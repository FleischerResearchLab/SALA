# AUTOGENERATED! DO NOT EDIT! File to edit: 00_SALA.ipynb (unless otherwise specified).

__all__ = ['SALA', 'remove_first_day']

# Cell
class SALA:
    """
    DataFrame-like storage for actiwatch data loaded either from a directory of csv files
    or an existing SALA or dataframe object.


        Attributes
        ----------
        data: pd.DataFrame
            Dataframe of processed timing data

        timezone: str
            Single timezone specified for all data within the object. A list of
            valid timezones can be obtained from pytz.all_timezones

        latitude: float
            Latitude position for sunrise/sunset calculations

        longitude: float
            Longitude position for sunrise/sunset calculations

        Methods
        -------
        init(data=None, directory=None, timezone=None, latitude=None, longitude=None)
            Initialization with a pre-parsed dataframe or raw data and other details

        get_raw_data(key, directory, grouping='Group')
            Loads raw actiwatch data

        export_timing_data(timing_data)
            Exports timing data to a parquet file

        process_timing_data(outfile, thresholds, directory, recalc_raw, recalc_timing, export_hook)
            Handle unprocessed data into two formats: a dataframe with all raw data, another with
            only processed timing data

        set_sun_timings(timing_data)
            Calculates sunset and sunrise timing for a single dataframe

        process_sleep_data
            Processes sleep data for existing timing data
    """

    def __init__(self, data=None, directory=None, timezone=None, latitude=None, longitude=None):
        """
        Initializes a SALA object either from existing parsed timing data, or from a directory
        of csvs. Timezone information can be optionally included to allow for sunset, sunrise
        data to be added.

        #### Parameters

            data: pd.DataFrame (optional)
                Pre-parsed dataframe.

            directory: dictionary (optional)
                Dictionary of valid folder names to load actiwatch data from.
                Folders should have .csv files in them.

            timezone: str (optional)
                A valid timezone (a list of timezones can be obtained from pytz.all_timezones).

            latitude: float (optional)
                Latitude position for sunrise/sunset calculations. Northern latitudes
                should be positive values.

            longitude: float (optional)
                Longitude position for sunrise/sunset calculations. Eastern longitudes
                should be positive values.
        """
        self._data = data
        self._directory = directory
        self._timezone = timezone
        self._latitude = latitude
        self._longitude = longitude

    @property
    def data(self):
        """Getter method for data."""
        return self._data

    @data.setter
    def data(self, value):
        """Setter method for data."""
        if type(value) != pd.DataFrame:
            raise TypeError("Error: Data must be of type pd.DataFrame")
        self._data = value

    @property
    def directory(self):
        """Getter method for directory."""
        return self._directory

    @directory.setter
    def directory(self, value):
        """Setter method for directory."""
        if type(value) != str:
            raise TypeError("Error: directory must be a valid string")
        self._directory = value

    @property
    def timezone(self):
        """Getter method for timezone."""
        return self._timezone

    @directory.setter
    def timezone(self, value):
        """Setter method for timezone."""
        if type(value) != str:
            raise TypeError("Error: timezone must be a valid string")
        self._timezone = value

    @property
    def latitude(self):
        """Getter method for latitude."""
        return self._latitude

    @directory.setter
    def latitude(self, value):
        """Setter method for latitude."""
        if not isinstance(value, (int, float, complex)):
            raise TypeError("Error: latitude must be a numeric")
        self._latitude = value

    @property
    def longitude(self):
        """Getter method for longitude."""
        return self._longitude

    @longitude.setter
    def longitude(self, value):
        """Setter method for longitude."""
        if not isinstance(value, (int, float, complex)):
            raise TypeError("Error: longitude must be a numeric")
        self._longitude = value

    def get_raw_data(self, key, directory = None, grouping = 'Group'):
        """Loads raw actiwatch data for a particular group based on a string key.

            #### Parameters

            key: str

                The key to load actiwatch data from (for example, "v1")

            directory: dict

                Dictionary of valid folders to load actiwatch data from.
                Folders should have .csv files in them. If no dictionary
                is provided, it uses the one initialized as part of the SALA
                object.

            grouping: str

                Name of the generated column for specifying groupings, where
                the values will be the name of the key given. Default = 'Group'.

            #### Returns

            All of the raw unprocessed data within the directory.

    """
        if directory is None and self._directory is None:
            raise ValueError("Error: a valid source of data must be provided.")
        if directory is not None:
            self._directory = directory
        raw_data = load_actiwatch_data(self.directory[key], uidprefix = key)[0]
        raw_data[grouping] = key
        return raw_data

    def export(self, timing_data, outfile):
        """
        Exports existing timing data to a parquet format.

        #### Parameters
            timing_data: pd.DataFrame

            Timing data

            outfile: str

                Directory to save to. (e.g. ../SALA/example_output/)
        """

        if self.data is None and timing_data is None:
            raise Exception("Error: no timing data available to export.")
        # putting date information in a parquet valid format
        timing_data["Date"] = timing_data["Date"].values.astype("datetime64[s]")
        timing_data.to_parquet(f"{outfile}timing.parquet",
                               engine = "fastparquet", compression="gzip")


    def process_data(self,
                     outfile,
                     thresholds,
                     calc_raw = False,
                     calc_timing = False,
                     export_hook = None):
        """Process existing timing and raw data dataframes by loading them
        from disk or calculating their values if specified.

        #### Parameters

        outfile: str

            File for re-written data to be placed in, or for data to be loaded from.

        thresholds: list

            List of light thresholds for the watch data.

        key: str

            The key to load actiwatch data from.

        calc_raw: bool

            Forces calculation process if true, loads processed data from disk otherwise.
            Default value is 'False'

        calc_timing: bool

            Forces calculation of light timing data and exports the resulting data,
            loads it from disk otherwise.
            Default value is 'False'

        export_hook: function

            Placeholder for user to use their own function during data processing.
            This function should take in the timing data as a parameter. See
            documentation for example.
        #### Returns

            (as a tuple of pd.DataFrames) all the data, the processed timing data
        """
        if calc_raw:
            print("Loading raw data from disk...")
            raw_results = (
            Parallel(n_jobs=len(self._directory))(delayed(self.get_raw_data)(key, self._directory) for key in self._directory.keys())
                   )
            all_data = pd.concat(raw_results)
            # save data to parquet file
            all_data.to_parquet(outfile + "raw.parquet", engine = 'fastparquet',
                               compression = "gzip")
        else:
            all_data = pd.read_parquet(outfile + "raw.parquet")

        if calc_timing:
            print("Processing light timing data...")
            timing_results = (Parallel(n_jobs=len(thresholds))
            (delayed(firstAndLastLight)(all_data, threshold) for threshold in thresholds)
                             )
            timing_data = pd.concat(timing_results)

            # loading federal holidays to classify dates as weekend/holiday
            cal = calendar()
            holidays = (
            cal.holidays(start = timing_data.Date.min(), end = timing_data.Date.max())
        )
            # retrieve day number (e.g. 0) from date index
            timing_data["DayofWeek"] = pd.DatetimeIndex(timing_data["Date"]).day_of_week
            days = ["Mon", "Tues", "Wed", "Thu", "Fri", "Sat", "Sun"]
            day_type = ["Weekday","Weekday","Weekday",
                    "Weekday","Weekday","Weekend/Holiday","Weekend/Holiday"]

            # result should be a combination of Group identifier and the day of the week (e.g. Mon)
            timing_data["GroupDayofWeek"] = (timing_data["Group"] + np.array(days)[timing_data["DayofWeek"]])

            is_holiday = pd.to_datetime(timing_data["Date"]).isin(holidays)
            weekends = (timing_data["Group"] + "Weekend/Holiday")

             # result should be a combination of Group identifier and day type (e.g. Weekday)
            day_types = (timing_data["Group"] + np.array(day_type)[timing_data["DayofWeek"]])

            timing_data["GroupDayType"] = day_types.where(~is_holiday).combine_first(weekends.where(is_holiday))
            timing_data["Weekend/Holiday"] = ((timing_data["DayofWeek"] > 4) | is_holiday)

            # function hook for extra processing before exporting to parquet
            if export_hook:
                timing_data = export_hook(timing_data)

            # setting and exporting timing data
            self._data = timing_data
            timing_data["Watch period"] = pd.to_timedelta(timing_data["Watch period"])
            self.export(timing_data, outfile)
        else:
            timing_data = pd.read_parquet(outfile + "timing.parquet", engine = "fastparquet")

        return all_data, timing_data

    def sun_timings(self, location, region):
        """Given a location (city) and region as additional markers,
        calculate sunset and sunrise timings.

        #### Parameters

        location: str (any string)

            Name of the location to lookup.

        region: str (any string)

            Region that the location is located in.

        #### Returns

            Modified timing data with sunrise and sunset calculations
        """

        if self._timezone is None or self._latitude is None or self._longitude is None:
            raise ValueError("Error: Missing timezone, latitude, or longitude info.")

        # add location info for calculating astral data
        city = LocationInfo(location, region, self._timezone, self._latitude, self._longitude)
        self._data["Sunrise"] = self._data["Date"].apply( lambda x: sun.sunrise(city.observer,
                                                                           x,
                                                                           tzinfo = city.tzinfo))
        self._data["Sunset"] = self._data["Date"].apply( lambda x: sun.sunset(city.observer,
                                                                         x,
                                                                         tzinfo = city.tzinfo))
        return self._data

    def process_sleep(self, sleep_split = "18:00", num_sleeps = 3):
        """Processes sleep data for existing timing data.

        #### Parameters

        timing_data: pd.DataFrame

            Timing data

        sleep_split: str

            Time to split the sleep day. Default is "18:00", which is 6:00PM.

        num_sleeps: int

            Cutoff for number of sleeps to display in first resulting frame.
            Default = 3, frame will store days with 3+ sleep instances

        #### Returns

            short_frame: pd.DataFrame

                Onset, offset, and duration for sleep periods on days with
                more than num_sleeps number of sleep periods

            timing_data: pd.DataFrame

                Modified timing data with included sleep information

        """
        sleepers = []
        sleep_onsets = []
        sleep_offsets = []
        sleep_durations = []
        sleep_onsetMSLMs = []
        sleep_offsetMSLMs = []

        timing_data = self._data
        for arow in timing_data.itertuples():
            UID = arow.UID
            DT = pd.to_datetime(arow.Date)
            TM = pd.to_datetime(DT + pd.Timedelta("1 day"))
            today = DT.strftime("%Y-%m-%d")

            nextday = TM.strftime("%Y-%m-%d")

            # taking raw timing data entry and splitting a "sleep day" at 6pm
            # under the assumption that people do not end their days that early
            day_split = all_data.query("UID == @UID").loc[today +" " + sleep_split:nextday + " 18:00"]

            # REST-S = watch thinks user is asleep
            asleep = day_split[ day_split["Interval Status"] == "REST-S"].copy()

            # there may be more than one sleep period in a given day's data
            # new sleep period = when there is more than 1 hour between successive REST-S entries
            sleep_periods = []
            per = 0
            count = 0

            try:
                lt = asleep.index[0]
                for time in asleep.index:
                    # allow up to 1 hour of being awake in the middle of the night
                    if (time - lt > pd.Timedelta("1 hour")):
                        per += 1
                    lt = time
                    sleep_periods.append(per)
                asleep["Sleep period"] = sleep_periods
            except IndexError:
                asleep["Sleep period"] = [pd.to_datetime(0)]

            try:
            # calc sleep onsets/offsets/duration for each period of sleep in a person-day of data
                sleeps = asleep.reset_index().groupby("Sleep period").apply( lambda x: pd.DataFrame({
                         "Sleep onset": [x.DateTime.min()],
                         "Sleep offset": [x.DateTime.max()],
                         "Sleep duration": [x.DateTime.max() - x.DateTime.min()]
                         }, index = x.DateTime.dt.normalize() ))
            # if the value is = 0 -> np.int64 (not a DateTime)
            except AttributeError:
                sleeps = asleep.reset_index().groupby("Sleep period").apply( lambda x: pd.DataFrame({
                 "Sleep onset": [pd.to_datetime(DT)],
                 "Sleep offset": [pd.to_datetime(DT)],
                 "Sleep duration": [pd.to_timedelta(x.DateTime.max() - x.DateTime.min())]
                 }))
            sleeps = sleeps.drop_duplicates().sort_values(by="Sleep duration", ascending = False)
            onset = sleeps.iloc[0]['Sleep onset']
            offset = sleeps.iloc[0]['Sleep offset']
            dur =  sleeps.iloc[0]['Sleep duration']

            # if onset is actually a datetime
            if not isinstance(onset, np.int64):
                onMSLM = (onset - DT).total_seconds() / 60.0

            # if offset is actually a datetime
            if not isinstance(offset, np.int64):
                offMSLM = np.maximum((offset - TM).total_seconds() / 60.0, 0.0)

            sleep_onsets.append(onset)
            sleep_offsets.append(offset)
            sleep_durations.append(dur)
            sleep_onsetMSLMs.append(onMSLM)
            sleep_offsetMSLMs.append(offMSLM)
            sleep_count = sleeps.shape[0]

            # adding to short_frame
            if sleep_count >= num_sleeps:
                sleeps['UID'] = UID
                sleeps['DT'] = DT
                sleeps.reset_index(drop = True).set_index(['UID','DT'])
                sleepers.append(sleeps)
        short_frame = (
                       pd.concat(sleepers).reset_index().drop('DateTime',axis=1)
                       .set_index(['UID','DT']).drop_duplicates()
                       )
        timing_data["Sleep onset"] = sleep_onsets
        timing_data["Sleep offset"] = sleep_offsets
        timing_data["Sleep duration"] = sleep_durations
        timing_data["Sleep onset MSLM"] = sleep_onsetMSLMs
        timing_data["Sleep offset MSLM"] = sleep_offsetMSLMs

        self._data = timing_data

        return short_frame, timing_data

# Cell
def remove_first_day(timing_data):
    """Example function hook for removing data for the first day
    where its obvious that light data is non-existent (NaT)

     #### Parameters

    timing_data: pd.DataFrame

        Timing data
    """
    data = (
    timing_data[(timing_data["Last Light"].apply(np.isnat) == False)
               & (timing_data["Date"] != timing_data["Date"].min())]
            )
    return data