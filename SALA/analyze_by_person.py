# AUTOGENERATED! DO NOT EDIT! File to edit: 00_analyze_by_person.ipynb (unless otherwise specified).

__all__ = ['get_raw_data', 'export_timing_data', 'process_timing_data', 'remove_first_day', 'set_sun_timings',
           'process_sleep_data']

# Cell
def get_raw_data(key:str, directory: dict, grouping:str = 'Group'):
    """Loads raw actiwatch data for a particular season.

    #### Parameters

    key: str

        The key to load actiwatch data from (for example, "v1")
    directory: dict

        Dictionary of valid folders to load actiwatch data from.
        Folders should have .csv files in them.
    grouping: str

        Name of the generated column for specifying groupings, where
        the values will be the name of the key given. Default = 'Group'

    """
    raw_data, summary_data = load_actiwatch_data(directory[key],uidprefix = key)
    raw_data['Group'] = key
    return raw_data

# Cell
def export_timing_data(timing_data):
    """ Exports timing data to parquet.

    #### Parameters

    timing_data: pd.DataFrame

        Timing data
    """
    timing_data.Date = timing_data.Date.values.astype("datetime64[s]")
    timing_data.to_parquet(
        outfile + "timing.parquet", engine = "fastparquet", compression = "gzip"
       )

# Cell
def process_timing_data(location: str,
                     outfile: str,
                     thresholds: list,
                     key: str,
                     directory: dict,
                     recalc_raw: bool = False,
                     recalc_timing: bool = False,
                     export_hook = None
                     ):
    """Setup timing and raw dataframes or recalculate their
    values if specified and necessary. Both operations take
    a long time and lots of memory. Ill-advised to recalculate
    timing specifically if the data has already been created.

    #### Parameters

    location: str

        Location for calculating light, for example 'Seattle'
    outfile: str

        File for re-written data to be placed in, or for data to be loaded from
    thresholds: list

        List of light thresholds for the watch data
    key: str

        The key to load actiwatch data from
    directory: dict

        Dictionary of valid seasons to retrieve actiwatch data from
    recalc_raw: bool

        Forces recalculation process if true, loads processed data from disk otherwise.
        Default value is 'False'
    recalc_timing: bool

        Forces recalculation of light timing data, loads it from disk otherwise.
        Default value is 'False'
    export_hook: function

        Placeholder for user to use their own function during data processing.
        This function should take in the timing data as a parameter. See
        documentation for example.
    #### Returns

        (as a tuple of pd.DataFrames) all the data, the timing data for a particular location
    """
    if recalc_raw:
        print("Loading raw data from disk...")
        raw_results = (
            Parallel(n_jobs=len(directory))(delayed(get_raw_data)(key, directory) for key in directory.keys())
                   )
        all_data = pd.concat(raw_results)
        # save data to parquet file
        all_data.to_parquet(outfile + "raw.parquet", engine = 'fastparquet',
                           compression = "gzip")
    else:
        # read data from parquet file
        all_data = pd.read_parquet(outfile + "raw.parquet")

    if recalc_timing:
        print("Calculating light timing data...")

        timing_results = (Parallel(n_jobs=len(thresholds))
            (delayed(firstAndLastLight)(all_data, threshold) for threshold in thresholds)
                      )
        timing_data = pd.concat(timing_results)
        print("Adding holiday markers to timing data...")
        cal = calendar()

        holidays = (
            cal.holidays(start = timing_data.Date.min(), end = timing_data.Date.max())
        )

        nn = pd.DatetimeIndex( timing_data.Date )
        timing_data["DayofWeek"] = nn.dayofweek
        days = ["Mon", "Tues", "Wed", "Thu", "Fri", "Sat", "Sun"]
        day_type = ["Weekday","Weekday","Weekday",
                    "Weekday","Weekday","Weekend/Holiday","Weekend/Holiday"]

        day_group = []
        dtp_group = []
        wknd_holiday = []

        # add days of week to data
        for index, row in timing_data.iterrows():
            day_group.append(row["Group"].split(location)[0] + days[row["DayofWeek"]])
            if holidays.isin([row["Date"]]).any():
                dtp_group.append(row['Group'].split(location)[0] + "Weekend/Holiday")
                wknd_holiday.append(True)
            else:
                dtp_group.append(
                    row["Group"].split(location)[0] + day_type[row["DayofWeek"]]
                )
                wknd_holiday.append(row['DayofWeek'] > 4)

        timing_data["GroupDayofWeek"] = day_group
        timing_data["GroupDayType"] = dtp_group
        timing_data["Weekend/Holiday"] = wknd_holiday

        # function hook for extra processing before exporting to parquet
        if export_hook:
            timing_data = export_hook(timing_data)

        timing_copy = timing_data.copy()
        timing_copy["Watch period"] = pd.to_timedelta(timing_copy["Watch period"])
        export_timing_data(timing_copy)
    else:
        timing_copy = pd.read_parquet(outfile + "timing.parquet", engine = "fastparquet")
        # return date to original format
        timing_copy.Date = timing_copy.Date.apply(lambda x: x.date())
        timing_data = timing_copy.copy()

    return all_data, timing_data

# Cell
def remove_first_day(timing_data):
    """Example function hook for removing data for the first day
    where its obvious that light data is non-existent (NaT)

     #### Parameters

    timing_data: pd.DataFrame

        Timing data
    """
    data = (
    timing_data[(timing_data["Last Light"].apply(np.isnat) == False)
               & (timing_data["Date"] != timing_data["Date"].min())]
            )
    return data

# Cell
def set_sun_timings(timing_data,
                    loc:str,
                    region: str,
                    timezone: str,
                    latitude: float,
                    longitude: float
                   ):
    """Given a location (city), calculate sunset and sunrise timings for the data

    #### Parameters

    timing_data: pd.DataFrame

        Timing data
    loc: str (any string)

        Name of location to lookup for sunrise/sunset calculations
    region: str (any string)

        Name of the region the location belongs to
    timezone: str

        the location's timezone (a list of timezones can be obtained from pytz.all_timezones)
    latitude: float

        Latitude position of the location for sunrise/sunset calculations
    longitude: float

        Longitude position of the location for sunrise/sunset calculations
    #### Returns

        Modified timing data with sunrise and sunset calculations

    """
    # add location info for calculating astral data
    city = LocationInfo(loc, region, timezone, latitude, longitude)

    timing_data["Sunrise"] = timing_data.Date.apply( lambda x: sun.sunrise(city.observer,
                                                                           x,
                                                                           tzinfo = city.tzinfo))

    timing_data["Sunset"] = timing_data.Date.apply( lambda x: sun.sunset(city.observer,
                                                                         x,
                                                                         tzinfo = city.tzinfo))


    return timing_data

# Cell
def process_sleep_data(timing_data, num_sleeps: int = 2):
    """Processes sleep data for existing timing data.

    #### Parameters

    timing_data: pd.DataFrame

        Timing data
    num_sleeps: int

        Cutoff for number of sleeps to display in first resulting frame.
        Default = 2, frame will store days with 3+ sleep instances

    #### Returns

        short_frame: pd.DataFrame

            Onset, offset, and duration for sleep periods on days with
            more than num_sleeps number of sleep periods
        timing_data: pd.DataFrame

            Modified timing data with included sleep information

    """
    sleepers = []
    sleep_onsets = []
    sleep_offsets = []
    sleep_durations = []
    sleep_onsetMSLMs = []
    sleep_offsetMSLMs = []
    for arow in timing_data.itertuples():
        UID = arow.UID
        DT = pd.to_datetime(arow.Date)
        TM = pd.to_datetime(DT + pd.Timedelta("1 day"))
        today = DT.strftime("%Y-%m-%d")

        nextday = TM.strftime("%Y-%m-%d")

        # taking raw timing data entry and splitting a "sleep day" at 6pm
        # under the assumption that people do not end their days that early
        day_split = all_data.query("UID == @UID").loc[today +" 18:00":nextday + " 18:00"]

        # REST-S = watch thinks user is asleep
        asleep = day_split[ day_split["Interval Status"] == "REST-S"].copy()

        # there may be more than one sleep period in a given day's data
        # new sleep period = when there is more than 1 hour between successive REST-S entries
        sleep_periods = []
        per = 0
        count = 0

        try:
            lt = asleep.index[0]
            for time in asleep.index:
                # allow up to 1 hour of being awake in the middle of the night
                if (time - lt > pd.Timedelta("1 hour")):
                    per += 1
                lt = time
                sleep_periods.append(per)
            asleep["Sleep period"] = sleep_periods
        except IndexError:
            asleep["Sleep period"] = [pd.to_datetime(0)]


        try:
        # calc sleep onsets/offsets/duration for each period of sleep in a person-day of data
            sleeps = asleep.reset_index().groupby("Sleep period").apply( lambda x: pd.DataFrame({
                     "Sleep onset": [x.DateTime.min()],
                     "Sleep offset": [x.DateTime.max()],
                     "Sleep duration": [x.DateTime.max() - x.DateTime.min()]
                     }, index = x.DateTime.dt.normalize() ))
        # if the value is = 0 -> np.int64 (not a DateTime)
        except AttributeError:
            sleeps = asleep.reset_index().groupby("Sleep period").apply( lambda x: pd.DataFrame({
             "Sleep onset": [pd.to_datetime(DT)],
             "Sleep offset": [pd.to_datetime(DT)],
             "Sleep duration": [pd.to_timedelta(x.DateTime.max() - x.DateTime.min())]
             }))
        sleeps = sleeps.drop_duplicates().sort_values(by="Sleep duration", ascending = False)
        onset = sleeps.iloc[0]['Sleep onset']
        offset = sleeps.iloc[0]['Sleep offset']
        dur =  sleeps.iloc[0]['Sleep duration']

        # if onset is actually a datetime
        if not isinstance(onset, np.int64):
            onMSLM = (onset - DT).total_seconds() / 60.0

        # if offset is actually a datetime
        if not isinstance(offset, np.int64):
            offMSLM = np.maximum((offset - TM).total_seconds() / 60.0, 0.0)

        sleep_onsets.append(onset)
        sleep_offsets.append(offset)
        sleep_durations.append(dur)
        sleep_onsetMSLMs.append(onMSLM)
        sleep_offsetMSLMs.append(offMSLM)
        sleep_count = sleeps.shape[0]

        # adding to short_frame
        if sleep_count > num_sleeps:
            sleeps['UID'] = UID
            sleeps['DT'] = DT
            sleeps.reset_index(drop = True).set_index(['UID','DT'])
            sleepers.append(sleeps)
    short_frame = (
                   pd.concat(sleepers).reset_index().drop('DateTime',axis=1)
                   .set_index(['UID','DT']).drop_duplicates()
                   )
    timing_data["Sleep onset"] = sleep_onsets
    timing_data["Sleep offset"] = sleep_offsets
    timing_data["Sleep duration"] = sleep_durations
    timing_data["Sleep onset MSLM"] = sleep_onsetMSLMs
    timing_data["Sleep offset MSLM"] = sleep_offsetMSLMs

    return short_frame, timing_data